"""An implementation of a the Adam optimizer."""
from typing import Callable
from tqdm import tqdm
import numpy as np


class Adam(object):
    """The Adam optimizer."""

    def __init__(self,
                 learning_rate: float=1e-4,
                 beta1: float=0.9,
                 beta2: float=0.999,
                 epsilon: float=1e-6) -> None:
        """
        Initialize a new Adam optimizer.

        Args:
            learning_rate: how fast to adjust the parameters (dW)

        Returns:
            None

        """
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        # set the history of loss evaluations to empty list
        self.loss_history = []

    def __repr__(self) -> str:
        """Return an executable string representation of this object."""
        return '{}(learning_rate={}, beta1={}, beta2={})'.format(*[
            self.__class__.__name__,
            self.learning_rate,
            self.beta1,
            self.beta2
        ])

    def __call__(self,
                 X: np.ndarray,
                 shape: tuple,
                 loss_grads: Callable,
                 iterations: int=1000,
                 callback: Callable=None):
        """
        Reduce the loss generated by X by moving it based on its gradient.

        Args:
            X: the input value to adjust to minimize loss
            shape: the shape to coerce X to
            loss_grads: a callable method that returns loss and gradients
                        given some input
            iterations: the number of iterations of optimization to perform
            callback: an optional callback method to receive image updates

        Returns:
            an optimized X about the loss and gradients given

        """
        # setup the local data structures for the optimization
        moment1 = np.zeros_like(X)
        moment2 = np.zeros_like(X)

        # reset the history of loss evaluations to empty list
        self.loss_history = []
        for i in tqdm(range(iterations)):
            # pass the input through the loss function and generate gradients
            loss_i, grads_i = loss_grads([X])
            # calculate the new values of the first and second moment
            moment1 = self.beta1 * moment1 + (1 - self.beta1) * grads_i
            moment2 = self.beta2 * moment2 + (1 - self.beta1) * grads_i**2
            # debias the moments to avoid bias towards 0 at the beginning
            debias1 = moment1 / (1 - self.beta1**(i+1))
            debias2 = moment2 / (1 - self.beta2**(i+1))
            # move the input based on the gradients and learning rate
            X -= self.learning_rate * debias1 / (debias2**0.5 + self.epsilon)
            # update the loss history with this loss value
            self.loss_history.append(loss_i)
            # pass the values to the callback if any
            if callable(callback):
                callback(X, i)

        return X


# explicitly define the outward facing API of this module
__all__ = [Adam.__name__]
